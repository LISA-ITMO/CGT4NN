{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ú–µ—Ç–æ–¥—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º —Ç–µ–æ—Ä–µ—Ç–∏–∫–æ-–∏–≥—Ä–æ–≤–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞\n",
    "\n",
    "- **–¢–µ–æ—Ä–∏—è**: –ò.–í.–¢–æ–º–∏–ª–æ–≤ ivan-tomilov3@yandex.ru\n",
    "- **–†–µ–∞–ª–∏–∑–∞—Ü–∏—è**: –ú.–ê.–ó–∞–π—Ü–µ–≤–∞ maria@tail-call.ru\n",
    "- **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ CUDA**: –ê.–ï.–ì—Ä–∏–≥–æ—Ä—å–µ–≤–∞ admin@linkennt.ru\n",
    "- **–†–µ–≤–∏–∑–∏—è**: 9\n",
    "\n",
    "- **–î—Ä—É–≥–∏–µ —Ä–µ–≤–∏–∑–∏–∏**:\n",
    "  - –° 1 –ø–æ 7: [–Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫](https://disk.yandex.ru/d/aZozDpBlzh_z1A)\n",
    "  - 8 –∏ –¥–∞–ª–µ–µ: [GitHub releases page](https://github.com/LISA-ITMO/CGT4NN/releases)\n",
    "<!-- please do not append text into this block -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DONE\n",
    "\n",
    "> Gradient dropout - –Ω–µ –≤—Å–µ –∏–≥—Ä–æ–∫–∏ –Ω–µ –≤—Å–µ–≥–¥–∞ –ø–æ–ª—É—á–∞—é—Ç –≤—ã–∏–≥—Ä—ã—à\n",
    "\n",
    "- [x] –ë–æ–ª—å—à–µ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—ë–≤! (—Ç–µ–ø–µ—Ä—å –∏—Ö 5)\n",
    "    - [x] –í —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö 100-200 –Ω–µ–π—Ä–æ–Ω–æ–≤\n",
    "    - [x] –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —á—Ç–æ-—Ç–æ –∫—Ä–æ–º–µ 3 —Å–ª–æ—ë–≤\n",
    "\n",
    "–î–ª—è —ç—Ç–æ–≥–æ –º—ã –≤–≤–æ–¥–∏–º –∫–ª–∞—Å—Å `AugmentedReLUNetworkMultilayer`, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç\n",
    "–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å `AugmentedReLUNetwork` –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º `hidden_layers_count`.\n",
    "\n",
    "- [x] –í–∑—è—Ç—å –¥–∞—Ç–∞—Å–µ—Ç —Å –ø–ª–æ—Ö–∏–º –ø–µ—Ä—Ñ–æ–º–∞–Ω—Å–æ–º\n",
    "    - [x] –ö–∞–∫–æ–π –∂–µ –∏–∑ –Ω–∏—Ö —Å –ø–ª–æ—Ö–∏–º –ø–µ—Ä—Ñ–æ–º–∞–Ω—Å–æ–º? –†–∞–∑–≤–µ –Ω–µ –≤—Å–µ?\n",
    "      - –Ø –¥—É–º–∞—é, —á—Ç–æ #4, –µ–≥–æ $R^2$ –Ω–∏–∂–µ -15\n",
    "    - $R^2$ —É–≤–µ–ª–∏—á–∏–ª—Å—è —Å -15 –¥–æ -2 üéâ\n",
    "- [x] $\\alpha \\in \\{ 1, 1.12, 2 \\}$\n",
    "- [x] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å sMAPE –≤–º–µ—Å—Ç–æ $R^2$\n",
    "\n",
    "## TODO\n",
    "\n",
    "- [ ] –ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –≤ SMAPe\n",
    "- [ ] –ï—Å–ª–∏ Y -> R —ç—Ç–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, —Ç–æ –ø–ª–æ—Ö–∞—è –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å\n",
    "\n",
    "Best Response: —à–∞–≥ –≤ —Å—Ç–æ—Ä–æ–Ω—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞\n",
    "\n",
    "–ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–∞–ª—å–Ω–µ–µ - —Ñ—É–Ω–∫—Ü–∏—è –æ—à–∏–±–∫–∏ —ç—Ç–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–π –≤–∏–¥ —Å–ª–æ—è\n",
    "\n",
    "–ü—Ä–∏–Ω–∏–º–∞—Ç –≤—ã—Ö–æ–¥ —Å–µ—Ç–∫–∏, –æ—Ç–¥–∞—ë—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç –ª–æ—Å—Å —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "\n",
    "–í —ç—Ç–æ–º —Å–ª—É—á–∞–µ —É –Ω–∞—Å –≤–µ—Å—å –º–µ—Ö–∞–Ω–∏–∑–º –∑–∞–º—ã–∫–∞–µ—Ç—Å—è, –∞ –∑–∞–º–∫–Ω—É—Ç–æ–π –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –∏ –æ–±—É—á–∞–µ–º–æ–π –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–∏\n",
    "\n",
    "- [ ] –ü—Ä–æ–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ –æ—Ç–∫—Ä—ã—Ç—É—é –∏–≥—Ä—É\n",
    "\n",
    "- [ ] –°—Ç–∞—Ç—å—è –ø—Ä–æ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é –º–µ–∂–¥—É –∞–≥–µ–Ω—Ç–∞–º–∏ –µ—Å—Ç—å —É –ò–≤–∞–Ω–∞\n",
    "\n",
    "- [ ] –ü–µ—Ä–µ–¥–µ–ª—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—É—é –∏–≥—Ä—É\n",
    "\n",
    "- –Ω–µ–∞—Ä—Ö–∏–º–µ–¥–æ–≤—ã –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, p-–∞–¥–∏—á–µ—Å–∫–∏–µ —á–∏—Å–ª–∞\n",
    "\n",
    "\n",
    "======\n",
    "\n",
    "- [ ] –Ω–µ –ø—Ä–∞–≤–æ –≥–æ–ª–æ—Å–∞, –∞ –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è\n",
    "\n",
    "- [ ] F1 Smape\n",
    "\n",
    "- [ ] —Å–¥–µ–ª–∞—Ç—å –∫ —Ç–∞–±–ª–∏—á–∫–∞–º –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–±–ª—é–¥–∞–µ–º —Ç–æ-—Ç–æ —Ç–æ-—Ç–æ\n",
    "\n",
    "## BACKLOG\n",
    "\n",
    "- [ ] –ü—Ä–∏–º–µ–Ω—è—Ç—å gradient dropout –∫ –ø–æ–ª–æ–≤–∏–Ω–µ —Å–µ—Ç–∏ –∏ –∫–æ –≤—Å–µ–π —Å–µ—Ç–∏\n",
    "- [ ] –í–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å —à—É–º –æ—Ç 0 –¥–æ 1 (–∞ –Ω–µ 2)\n",
    "- [ ] –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –∫–∞–∫–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π —Ä–µ—à–∞–ª–∏ MNIST character recognition –Ω–∞ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–µ—Ç—è—Ö\n",
    "    - [ ] Reference: https://github.com/ranimeshehata/Feed-Forward-Neural-Network-on-MNIST\n",
    "\n",
    "- [ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é *He* –≤–º–µ—Å—Ç–æ *Xavier*\n",
    "  - [ ] –°—Ä–∞–≤–Ω–∏—Ç—å\n",
    "- [ ] –ö–∞–∫ –æ—Ü–µ–Ω–∏—Ç—å —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —Å–ª—É—á–∞–π–Ω–æ–π –≤–µ–ª–∏—á–∏–Ω—ã?\n",
    "- [ ] –ù–∞ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–º–µ–Ω—è—Ç—å –∫–ª–∞—Å—Å $q$ –Ω–∞ –∫–ª–∞—Å—Å $r$ —Å –∑–∞–¥–∞–Ω–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é, –∞ –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —à—É–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric Mean Absolute Percentage Error (sMAPE)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error\n",
    "\n",
    "Common in time series forecasting\n",
    "\n",
    "$$ \\mathrm{SMAPE} = {100 \\over n}  \\sum_{t=1}^{n} {{ |F_t - A_t| }\\over{ (|A_t| + |F_t|)/2 }} $$\n",
    "\n",
    "- $A_t$ - actual value\n",
    "- $F_t$ - forecast value\n",
    "\n",
    "Implementation of sMAPE is avaiable in the `torchmetrics` library:\n",
    "https://torchmetrics.readthedocs.io/en/v1.0.0/regression/symmetric_mean_absolute_percentage_error.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2290)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchmetrics import SymmetricMeanAbsolutePercentageError\n",
    "\n",
    "target = torch.tensor([1, 10, 1e6])\n",
    "preds = torch.tensor([0.9, 15, 1.2e6])\n",
    "\n",
    "smape = SymmetricMeanAbsolutePercentageError()\n",
    "smape(preds, target) # tensor(0.2290)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to $R^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583327812227765"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(preds, target) # 0.9583327812227765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset #3 {\n",
       "   name: \"StudentPerformanceFactors\"\n",
       "   learning_task: LearningTask(name='regression', criterion=MSELoss(), y_dtype=torch.float32)\n",
       "   classes_count: 1\n",
       "   target: \"Exam_Score\"\n",
       "   _data: None\n",
       " },\n",
       " Dataset #6 {\n",
       "   name: \"wine_quality\"\n",
       "   learning_task: LearningTask(name='regression', criterion=MSELoss(), y_dtype=torch.float32)\n",
       "   classes_count: 1\n",
       "   target: \"quality\"\n",
       "   _data: None\n",
       " },\n",
       " Dataset #9 {\n",
       "   name: \"294_satellite_image\"\n",
       "   learning_task: LearningTask(name='regression', criterion=MSELoss(), y_dtype=torch.float32)\n",
       "   classes_count: 1\n",
       "   target: \"target\"\n",
       "   _data: None\n",
       " },\n",
       " Dataset #10 {\n",
       "   name: \"573_cpu_act\"\n",
       "   learning_task: LearningTask(name='regression', criterion=MSELoss(), y_dtype=torch.float32)\n",
       "   classes_count: 1\n",
       "   target: \"target\"\n",
       "   _data: None\n",
       " }]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cgtnnlib.LearningTask import is_regression_task\n",
    "import cgtnnlib.datasets as ds\n",
    "\n",
    "list(filter(lambda x: is_regression_task(x.learning_task), ds.datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setup complete'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Callable\n",
    "from cgtnnlib.Dataset import Dataset\n",
    "from cgtnnlib.constants import LEARNING_RATE, RANDOM_STATE\n",
    "import cgtnnlib.training as tr\n",
    "import cgtnnlib.datasets as ds\n",
    "from cgtnnlib.NoiseGenerator import NoiseGenerator, target_dispersion_scaled_noise, stable_noise, no_noise_generator\n",
    "\n",
    "iterations = 10\n",
    "\"–ß–∏—Å–ª–æ –º–æ–¥–µ–ª–µ–π –≤ –∫–∞–∂–¥–æ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ\"\n",
    "\n",
    "epochs = 10\n",
    "\"–ß–∏—Å–ª–æ —ç–ø–æ—Ö\"\n",
    "\n",
    "pp = [0.0, 0.5, 0.9]\n",
    "\"–ù–∞–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ gradient dropout\"\n",
    "\n",
    "hidden_layers_count = 5\n",
    "\"–ß–∏—Å–ª–æ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—ë–≤\"\n",
    "\n",
    "inner_layer_size = 150\n",
    "\"–ß–∏—Å–ª–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö\"\n",
    "\n",
    "datasets = list(filter(\n",
    "    lambda x: is_regression_task(x.learning_task),\n",
    "    ds.datasets\n",
    "))\n",
    "\"–ù–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–æ–≤–æ–¥–∏—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç\"\n",
    "\n",
    "#output_dir = 'rev9' #3 layers\n",
    "#output_dir = 'rev9.2' #5 layers\n",
    "output_dir = 'rev9.3' #5 layers, \n",
    "\"–ü–∞–ø–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\"\n",
    "\n",
    "def dir_path(d: Dataset, p: float, n: NoiseGenerator):\n",
    "    \"–ü—É—Ç—å –∫–∞—Ç–∞–ª–æ–≥–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\"\n",
    "    return f'{output_dir}/dataset{d.number}_p{p}_noise{n.name}/'\n",
    "\n",
    "def model_path(d: Dataset, p: float, n: NoiseGenerator, N: int):\n",
    "    \"–ü—É—Ç—å —Ñ–∞–π–ª–∞ PyTorch –º–æ–¥–µ–ª–∏ (`.pth`)\"\n",
    "    return f'{output_dir}/dataset{d.number}_p{p}_noise{n.name}/{N}.pth'\n",
    "\n",
    "def report_filename(d: Dataset, p: float, n: NoiseGenerator, N: int):\n",
    "    \"–ò–º—è —Ñ–∞–π–ª–∞ –æ—Ç—á—ë—Ç–∞ (`.json`, —Å–º. `cgtnnlib/Report.py`)\"\n",
    "    return f'dataset{d.number}_p{p}_noise{n.name}/{N}.json'\n",
    "\n",
    "def report_path(d: Dataset, p: float, n: NoiseGenerator, N: int):\n",
    "    \"–ü—É—Ç—å —Ñ–∞–π–ª–∞ –æ—Ç—á—ë—Ç–∞ (`.json`, —Å–º. `cgtnnlib/Report.py`)\"\n",
    "    return f'{output_dir}/{report_filename(d, p, n, N)}'\n",
    "\n",
    "\n",
    "ng_makers: list[Callable[[Dataset], NoiseGenerator]] = [\n",
    "    lambda _: no_noise_generator,\n",
    "    lambda d: target_dispersion_scaled_noise(\n",
    "        dataset=d,\n",
    "        factor=0.03,\n",
    "        random_seed=RANDOM_STATE + 1,\n",
    "    ),\n",
    "    lambda d: stable_noise(\n",
    "        dataset=d,\n",
    "        factor=0.03,\n",
    "        alpha=1,\n",
    "        beta=0,\n",
    "    ),\n",
    "    lambda d: stable_noise(\n",
    "        dataset=d,\n",
    "        factor=0.03,\n",
    "        alpha=1.12,\n",
    "        beta=0,\n",
    "    ),\n",
    "    lambda d: stable_noise(\n",
    "        dataset=d,\n",
    "        factor=0.03,\n",
    "        alpha=2.0,\n",
    "        beta=1,\n",
    "    ),\n",
    "]\n",
    "\"–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã —à—É–º–æ–≤ –¥–ª—è –ø—Ä–∏–º–µ—à–∏–≤–∞–Ω–∏—è –∫–æ –≤—Ö–æ–¥–∞–º\"\n",
    "\n",
    "'setup complete'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–Ø –Ω–µ –ø–æ–Ω–∏–º–∞—é, –ø–æ—á–µ–º—É –º—ã –ø—Ä–∏–º–µ—à–∏–≤–∞–µ–º —à—É–º –∏–º–µ–Ω–Ω–æ –∫–æ –≤—Ö–æ–¥–∞–º.\n",
    "–ú–æ–∂–µ—Ç, —Ç–∞–º –∏ –Ω–µ —Ç–∞–∫ –≤—Å—ë –¥–µ–ª–∞–µ—Ç—Å—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Model B\n",
    "\n",
    "- both take ~5m 06s to train 10 iterations\n",
    "- both on both noises: about ~10m\n",
    "\n",
    "<hr>\n",
    "\n",
    "- `ds.datasets['StudentPerformanceFactors']` takes ~2m 30s to train 10 iterations\n",
    "- `ds.datasets['wine_quality']` takes ~2m 36s to train 10 iterations\n",
    "\n",
    "<hr>\n",
    "\n",
    "- `ds.datasets['allhyper']` takes ~36m to train on all noise generators\n",
    "\n",
    "### Model B*\n",
    "\n",
    "3 hidden layers:\n",
    "- ~15m 38s: Dataset #3, 5 ng_makers, $p \\in \\{ 0.0, 0.5, 0.9 \\} $ \n",
    "- ~9m 30s: Dataset #3, 5 ng_makers, $p \\in \\{ 0.0, 0.5, 0.9 \\} $ \n",
    "\n",
    "5 hidden layers:\n",
    "- ~14m 10s: Dataset #3, 5 ng_makers, $p \\in \\{ 0.0, 0.5, 0.9 \\} $ \n",
    "\n",
    "- ~93m 23s: Datasets {#3, #6, #9, #10}, 5 ng_makers, $p \\in \\{ 0.0, 0.5, 0.9 \\} $ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=9 #10 gStable10A2.0B1F0.03 p=0.9 E9/10 S547 Loss=31710.1895@AugmentedReLUNetworkMultilayer\n",
      "create_and_train_model(): saved model to rev9.3/dataset10_p0.9_noiseStable10A2.0B1F0.03/9.pth\n",
      "Report saved to rev9.3/dataset10_p0.9_noiseStable10A2.0B1F0.03/9.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'training complete'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training\n",
    "\n",
    "import os\n",
    "\n",
    "from cgtnnlib.Report import Report\n",
    "from cgtnnlib.nn.AugmentedReLUNetworkMultilayer import AugmentedReLUNetworkMultilayer\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    for ng_maker in ng_makers:\n",
    "        for p in pp:\n",
    "            noise_generator = ng_maker(dataset)\n",
    "            for iteration in range(iterations):\n",
    "                os.makedirs(dir_path(dataset, p, noise_generator), exist_ok=True)\n",
    "                report = Report(\n",
    "                    dir=output_dir,\n",
    "                    filename=report_filename(d=dataset, p=p,\n",
    "                                             n=noise_generator, N=iteration)\n",
    "                )\n",
    "                tr.super_train_model(\n",
    "                    make_model=lambda: AugmentedReLUNetworkMultilayer(\n",
    "                        inputs_count=dataset.features_count,\n",
    "                        outputs_count=dataset.classes_count,\n",
    "                        p=p,\n",
    "                        inner_layer_size=inner_layer_size,\n",
    "                        hidden_layers_count=hidden_layers_count,\n",
    "                    ),\n",
    "                    model_path=model_path(d=dataset, p=p,\n",
    "                                          n=noise_generator, N=iteration),\n",
    "                    dataset=dataset,\n",
    "                    report=report,\n",
    "                    epochs=epochs,\n",
    "                    learning_rate=LEARNING_RATE,\n",
    "                    dry_run=False,\n",
    "                    iteration=iteration,\n",
    "                    noise_generator=noise_generator,\n",
    "                )\n",
    "\n",
    "'training complete'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation & Analysis\n",
    "\n",
    "~2m 44s: Dataset #3, 5 ng_makers, $p \\in \\{ 0.0, 0.5, 0.9 \\} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report found at rev9/dataset4_p0.0_noiseNoNoise/0.json. Loading...\n",
      "Report loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:137: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:137: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:137: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:137: SyntaxWarning: invalid escape sequence '\\{'\n",
      "/var/folders/m9/y193wddj505gjbgjyvhjmzjm0000gq/T/ipykernel_28685/331120151.py:137: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  fig.suptitle(f'–î–∞—Ç–∞—Å–µ—Ç #{dataset.number}: {dataset.name}\\{model_type}', fontsize=16)\n",
      "/var/folders/m9/y193wddj505gjbgjyvhjmzjm0000gq/T/ipykernel_28685/331120151.py:137: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  fig.suptitle(f'–î–∞—Ç–∞—Å–µ—Ç #{dataset.number}: {dataset.name}\\{model_type}', fontsize=16)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AugmentedReLUNetworkMultilayer:\n\tMissing key(s) in state_dict: \"layers.8.weight\", \"layers.8.bias\", \"layers.10.weight\", \"layers.10.bias\". \n\tsize mismatch for layers.6.weight: copying a param with shape torch.Size([1, 150]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for layers.6.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([150]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 116\u001b[0m\n\u001b[1;32m     94\u001b[0m         plot_deviant_curves_on_ax_or_plt(\n\u001b[1;32m     95\u001b[0m             ax_or_plt\u001b[38;5;241m=\u001b[39max,\n\u001b[1;32m     96\u001b[0m             models\u001b[38;5;241m=\u001b[39m[{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m             quantiles_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m    110\u001b[0m         )\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m draw_ax\n\u001b[1;32m    114\u001b[0m ax_drawers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    115\u001b[0m     [\n\u001b[0;32m--> 116\u001b[0m         [\u001b[43mmake_ax_drawer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mng_maker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pp]\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ng_maker \u001b[38;5;129;01min\u001b[39;00m ng_makers\n\u001b[1;32m    118\u001b[0m     ]\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets\n\u001b[1;32m    120\u001b[0m ]\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(ax_drawers)\n\u001b[1;32m    123\u001b[0m (nrows, ncols) \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mlen\u001b[39m(ax_drawers[\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mlen\u001b[39m(ax_drawers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m    126\u001b[0m )\n",
      "Cell \u001b[0;32mIn[18], line 85\u001b[0m, in \u001b[0;36mmake_ax_drawer\u001b[0;34m(read_json, dataset, ng_maker, p)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# metric = 'loss'\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# metric = 'mse'\u001b[39;00m\n\u001b[1;32m     82\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmape\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     84\u001b[0m files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 85\u001b[0m     \u001b[43mread_eval_from_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations)\n\u001b[1;32m     87\u001b[0m ]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreport_patha(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     91\u001b[0m curve \u001b[38;5;241m=\u001b[39m summarize_series_list([file[metric] \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files])\n",
      "Cell \u001b[0;32mIn[18], line 66\u001b[0m, in \u001b[0;36mmake_ax_drawer.<locals>.read_eval_from_iteration\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     58\u001b[0m model \u001b[38;5;241m=\u001b[39m model_type(\n\u001b[1;32m     59\u001b[0m     inputs_count\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mfeatures_count,\n\u001b[1;32m     60\u001b[0m     outputs_count\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mclasses_count,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     hidden_layers_count\u001b[38;5;241m=\u001b[39mhidden_layers_count,\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 66\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_patha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m super_eval_model(\n\u001b[1;32m     68\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m     69\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     70\u001b[0m     report\u001b[38;5;241m=\u001b[39mreport,\n\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_eval_from_iteration\u001b[39m\u001b[38;5;124m'\u001b[39m, path, n)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AugmentedReLUNetworkMultilayer:\n\tMissing key(s) in state_dict: \"layers.8.weight\", \"layers.8.bias\", \"layers.10.weight\", \"layers.10.bias\". \n\tsize mismatch for layers.6.weight: copying a param with shape torch.Size([1, 150]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for layers.6.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([150])."
     ]
    }
   ],
   "source": [
    "## Analysis\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cgtnnlib.Report import Report\n",
    "from cgtnnlib.LearningTask import is_classification_task\n",
    "from cgtnnlib.analyze import plot_deviant_curves_on_ax_or_plt\n",
    "from cgtnnlib.constants import NOISE_FACTORS\n",
    "from cgtnnlib.evaluate import eval_report_at_path, super_eval_model\n",
    "from cgtnnlib.nn.AugmentedReLUNetworkMultilayer import AugmentedReLUNetworkMultilayer\n",
    "\n",
    "\n",
    "model_type = AugmentedReLUNetworkMultilayer\n",
    "\n",
    "\n",
    "\n",
    "def read_json(path: str) -> dict:\n",
    "    with open(path) as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def summarize_series_list(series_list: list[pd.Series]):\n",
    "    df = pd.DataFrame(series_list).T\n",
    "\n",
    "    summary_df = pd.DataFrame({\n",
    "        0.25: df.quantile(0.25, axis=1),\n",
    "        0.75: df.quantile(0.75, axis=1),\n",
    "        'mean': df.mean(axis=1),\n",
    "    })\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def make_ax_drawer(\n",
    "    read_json,\n",
    "    dataset,\n",
    "    ng_maker,\n",
    "    p,\n",
    "):\n",
    "    noise_generator = ng_maker(dataset)\n",
    "\n",
    "    def report_patha(N):\n",
    "        return report_path(dataset, p, noise_generator, N)\n",
    "\n",
    "    def model_patha(N):\n",
    "        return model_path(dataset, p, noise_generator, N)\n",
    "\n",
    "    def read_eval_from_iteration(n) -> pd.DataFrame:\n",
    "        path = report_patha(n)\n",
    "        report = Report.from_path(path)\n",
    "        # model_path = model_path(n)\n",
    "        \n",
    "        model = model_type(\n",
    "            inputs_count=dataset.features_count,\n",
    "            outputs_count=dataset.classes_count,\n",
    "            p=p,\n",
    "            inner_layer_size=inner_layer_size,\n",
    "            hidden_layers_count=hidden_layers_count,\n",
    "        )\n",
    "        model.eval()\n",
    "        model.load_state_dict(torch.load(model_patha(n)))\n",
    "        super_eval_model(\n",
    "            dataset=dataset,\n",
    "            model=model,\n",
    "            report=report,\n",
    "        )\n",
    "        print('read_eval_from_iteration', path, n)\n",
    "        return pd.DataFrame(read_json(path)['eval'])\n",
    "    \n",
    "    def read_loss_from_iteration(n) -> pd.DataFrame:\n",
    "        path = report_patha(n)\n",
    "        json = read_json(path)\n",
    "        return pd.DataFrame({ 'loss': json['loss'] })\n",
    "\n",
    "    # metric = 'loss'\n",
    "    # metric = 'mse'\n",
    "    metric = 'smape'\n",
    "\n",
    "    files = [\n",
    "        read_eval_from_iteration(n)\n",
    "        for n in range(iterations)\n",
    "    ]\n",
    "            \n",
    "    print(f'Processing {report_patha(0)}...')\n",
    "\n",
    "    curve = summarize_series_list([file[metric] for file in files])\n",
    "\n",
    "    def draw_ax(ax):\n",
    "        plot_deviant_curves_on_ax_or_plt(\n",
    "            ax_or_plt=ax,\n",
    "            models=[{\n",
    "                'curve': curve,\n",
    "                'color': 'purple',\n",
    "                'label': '–°—Ä–µ–¥–Ω–µ–µ',\n",
    "                'quantiles_color': 'pink',\n",
    "                'quantiles_label': '–ö–≤–∞—Ä—Ç–∏–ª–∏ 0,25; 0,75', \n",
    "            }],\n",
    "            X=curve.index,\n",
    "            title='\\n'.join([\n",
    "                f'{noise_generator.name}, p = {p}',\n",
    "            ]),\n",
    "            xlabel='–ò—Ç–µ—Ä–∞—Ü–∏—è',\n",
    "            ylabel=metric,\n",
    "            quantiles_alpha=0.5,\n",
    "        )\n",
    "    \n",
    "    return draw_ax\n",
    "\n",
    "ax_drawers = [\n",
    "    [\n",
    "        [make_ax_drawer(read_json, dataset, ng_maker, p) for p in pp]\n",
    "        for ng_maker in ng_makers\n",
    "    ]\n",
    "    for dataset in datasets\n",
    "]\n",
    "\n",
    "print(ax_drawers)\n",
    "(nrows, ncols) = (\n",
    "    len(ax_drawers[0]),\n",
    "    len(ax_drawers[0][0]),\n",
    ")\n",
    "\n",
    "print(f'Size: {nrows}x{ncols}')\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 40))\n",
    "\n",
    "    for j in range(nrows):\n",
    "        for k in range(ncols):\n",
    "            ax_drawers[i][j][k](axes[j, k])\n",
    "    \n",
    "    fig.suptitle(f'–î–∞—Ç–∞—Å–µ—Ç #{dataset.number}: {dataset.name}\\{model_type}', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.01, 1, 0.95]) # rect adjusts space for suptitle\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying 5 layers\n",
    "\n",
    "Smape goes down (p = 0):\n",
    "- 0.1 -> 0.08\n",
    "- 0.10 -> 0.085\n",
    "- 0.10 -> 0.085\n",
    "- 0.09 ~> 0.095 (goes up on Stble4A.12B0F0.03)\n",
    "- 0.11 -> 0.085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.abs(np.array([\n",
    "    0.1 - 0.08,\n",
    "    0.1 - 0.085,\n",
    "    0.10 - 0.085,\n",
    "    0.09 - 0.095,\n",
    "    0.11 - 0.085,\n",
    "])).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we gain ~0.016 of sMAPE for adding 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1322.8285714285714"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 610 / 350 * 759\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Chambers, J. M., Mallows, C. L., & Stuck, B. W. (1976). A method for simulating stable random variables. *Journal of the American Statistical Association*, *71*(354), 340-344.\n",
    "2. M. Firouzi, A. Mohammadpour. A Survey on Simulating Stable Random Variables. URL: https://www.semanticscholar.org/reader/11a1e93642dc0a5c94e6906bcca5e4d25d4e9d46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
